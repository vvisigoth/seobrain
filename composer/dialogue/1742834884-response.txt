I'll update `articlegenerator.py` to use the RAG (Retrieval-Augmented Generation) functionality from `search.py` to enhance article generation with relevant research from a knowledge base of documents.

Here's the implementation:

```python
#!python3.12
"""
SEO Generation Script with RAG (Retrieval-Augmented Generation)

This script takes a text file where each line is an SEO keyword,
searches a knowledge base for relevant information using RAG,
and generates optimized content that covers multiple keywords in each article.
"""

import os
import sys
import argparse
import json
import requests
import random
from pathlib import Path
from typing import List, Dict, Any, Optional

# Import RAG components from search.py
from langchain_community.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# Default settings
DEFAULT_MODEL = "anthropic/claude-3.7-sonnet"
DEFAULT_PROVIDER = "openrouter"
SERVER_URL = "http://localhost:5555"
OUTPUT_DIR = "seo_content"
DEFAULT_ARTICLE_COUNT = 50
DEFAULT_PREPROMPT_FILE = "preprompt.txt"
DEFAULT_KNOWLEDGE_DIR = "knowledge_base"
DEFAULT_INDEX_NAME = "seo_index"

def load_config():
    """Load API keys from config file"""
    try:
        with open('config.json', 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

def initialize_embeddings(config):
    """Initialize embeddings model based on available API keys"""
    if config.get("openai_api_key"):
        return OpenAIEmbeddings(api_key=config.get("openai_api_key"))
    else:
        # Fallback to local model that doesn't require API keys
        return HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

def load_documents(knowledge_dir: str) -> List[Document]:
    """Load all text files from knowledge directory"""
    documents = []
    knowledge_path = Path(knowledge_dir)
    
    if not knowledge_path.exists():
        print(f"Warning: Directory '{knowledge_dir}' does not exist. Creating it.")
        knowledge_path.mkdir(exist_ok=True)
        return documents
    
    # Find all text files recursively
    file_paths = []
    for ext in [".txt", ".md", ".html", ".json", ".csv"]:
        file_paths.extend(glob.glob(os.path.join(knowledge_dir, f"**/*{ext}"), recursive=True))
    
    for file_path in file_paths:
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
            doc = Document(
                page_content=content,
                metadata={
                    "source": file_path,
                    "filename": os.path.basename(file_path),
                }
            )
            documents.append(doc)
            
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
    
    print(f"Loaded {len(documents)} documents from {knowledge_dir}")
    return documents

def create_or_load_index(documents: List[Document], embeddings, index_name: str, rebuild: bool = False):
    """Create or load vector index"""
    # Check if we should rebuild the index
    if rebuild and os.path.exists(index_name):
        import shutil
        print(f"Rebuilding index - removing {index_name}")
        shutil.rmtree(index_name)
    
    # Check if index already exists
    if os.path.exists(index_name) and os.path.isdir(index_name):
        print(f"Loading existing index from {index_name}")
        try:
            vector_db = FAISS.load_local(index_name, embeddings)
            return vector_db
        except Exception as e:
            print(f"Error loading index: {e}")
            print("Creating new index...")
    
    # If no documents, return None
    if not documents:
        print("No documents to index")
        return None
    
    # Create chunks for better retrieval
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    
    # Create new index
    print(f"Creating new index with {len(chunks)} chunks")
    vector_db = FAISS.from_documents(chunks, embeddings)
    
    # Save index
    vector_db.save_local(index_name)
    return vector_db

def search_documents(query: str, vector_db, k: int = 5) -> List[Document]:
    """Search for relevant documents"""
    if vector_db is None:
        return []
    
    results = vector_db.similarity_search(query, k=k)
    return results

def format_research_results(results: List[Document]) -> str:
    """Format search results for inclusion in the prompt"""
    if not results:
        return "No relevant research found."
    
    output = ["### Relevant Research Information:"]
    
    for i, doc in enumerate(results):
        metadata = doc.metadata
        source = metadata.get("source", "Unknown source")
        
        # Add source information
        output.append(f"\n#### Source {i+1}: {os.path.basename(source)}")
        
        # Add content
        content = doc.page_content
        output.append(content)
    
    return "\n".join(output)

def load_keywords(file_path: str) -> List[str]:
    """Load keywords from a text file, one per line."""
    with open(file_path, 'r') as f:
        return [line.strip() for line in f if line.strip()]

def load_preprompt(file_path: str) -> str:
    """Load custom preprompt from a text file."""
    if not os.path.exists(file_path):
        return ""
    
    with open(file_path, 'r') as f:
        return f.read().strip()

def distribute_keywords(keywords: List[str], article_count: int) -> List[List[str]]:
    """
    Distribute all keywords across the specified number of articles.
    Each article will include multiple keywords.
    """
    # Ensure we don't create more articles than keywords
    article_count = min(article_count, len(keywords))
    
    # Shuffle keywords to randomize distribution
    shuffled_keywords = keywords.copy()
    random.shuffle(shuffled_keywords)
    
    # Calculate minimum keywords per article
    min_keywords_per_article = len(shuffled_keywords) // article_count
    remaining = len(shuffled_keywords) % article_count
    
    # Distribute keywords to articles
    articles = []
    index = 0
    for i in range(article_count):
        # Add one extra keyword to some articles if there are remainders
        count = min_keywords_per_article + (1 if i < remaining else 0)
        article_keywords = shuffled_keywords[index:index + count]
        articles.append(article_keywords)
        index += count
    
    return articles

def generate_seo_content(keywords: List[str], research: str, preprompt: str, model: str, provider: str, server_url: str) -> str:
    """Generate SEO content for a given set of keywords using the AI API with research data."""
    
    # Format the keywords for the prompt
    keywords_str = ", ".join([f'"{kw}"' for kw in keywords])
    primary_keyword = keywords[0]  # Use the first keyword as primary
    
    # Craft the prompt for SEO content generation
    base_prompt = f"""
Generate high-quality SEO content that incorporates ALL of the following keywords: {keywords_str}

Use "{primary_keyword}" as the primary focus, but naturally integrate all the other keywords throughout the article.

I've provided some research information below that may be relevant to these keywords. Use this information to make the 
article more authoritative, accurate, and valuable to readers. Incorporate relevant facts, statistics, and insights 
from the research where appropriate.

{research}

Please include:
1. A compelling H1 title (using markdown # syntax) that includes the primary keyword
2. A meta description (150-160 characters) that mentions 2-3 of the keywords
3. 5-7 relevant H2 subheadings (using markdown ## syntax) that incorporate different keywords
4. 800-1200 words of well-structured, informative content that naturally includes all keywords
5. A natural keyword density without keyword stuffing
6. A clear call-to-action at the end

The content should be engaging, informative, and optimized for search engines while providing genuine value to readers.
Ensure the article flows naturally and doesn't feel like it's artificially cramming in keywords.
"""

    # Combine preprompt with base prompt if preprompt exists
    prompt = f"{preprompt}\n\n{base_prompt}" if preprompt else base_prompt

    # Prepare request data
    request_data = {
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 4000,
        "temperature": 0.7,
        "provider": provider,
        "model": model
    }

    # Send request to server
    try:
        response = requests.post(
            f"{server_url}/api/generate",
            json=request_data,
            headers={"Content-Type": "application/json"}
        )

        if response.status_code != 200:
            raise Exception(f"HTTP error: {response.status_code} - {response.text}")

        return response.json()["content"]
    
    except requests.exceptions.RequestException as e:
        raise Exception(f"Connection error: {str(e)}")

def save_content(primary_keyword: str, all_keywords: List[str], content: str, output_dir: str) -> str:
    """Save the generated content to a file and return the file path."""
    # Create output directory if it doesn't exist
    Path(output_dir).mkdir(exist_ok=True)
    
    # Create a filename from the primary keyword
    filename = primary_keyword.lower().replace(' ', '_').replace("'", "").replace('"', '')
    # Remove any special characters that might cause issues with filenames
    filename = ''.join(c for c in filename if c.isalnum() or c == '_' or c == '-')
    filepath = os.path.join(output_dir, f"{filename}.md")
    
    # Add a header comment with all keywords used
    keywords_comment = "<!--\nKeywords covered in this article:\n"
    for kw in all_keywords:
        keywords_comment += f"- {kw}\n"
    keywords_comment += "-->\n\n"
    
    # Save the content with the keywords comment
    with open(filepath, 'w') as f:
        f.write(keywords_comment + content)
    
    return filepath

def main():
    """Main function to run the SEO generation script with RAG."""
    parser = argparse.ArgumentParser(description="Generate SEO content from keywords with RAG")
    parser.add_argument("input_file", help="Text file containing keywords, one per line")
    parser.add_argument("-o", "--output", default=OUTPUT_DIR, help=f"Output directory (default: {OUTPUT_DIR})")
    parser.add_argument("-m", "--model", default=DEFAULT_MODEL, help=f"AI model to use (default: {DEFAULT_MODEL})")
    parser.add_argument("-p", "--provider", default=DEFAULT_PROVIDER, 
                        choices=["ollama", "openai", "openrouter", "anthropic"],
                        help=f"AI provider (default: {DEFAULT_PROVIDER})")
    parser.add_argument("-s", "--server", default=SERVER_URL, help=f"Server URL (default: {SERVER_URL})")
    parser.add_argument("-n", "--num-articles", type=int, default=DEFAULT_ARTICLE_COUNT,
                        help=f"Number of articles to generate (default: {DEFAULT_ARTICLE_COUNT})")
    parser.add_argument("--preprompt", default=DEFAULT_PREPROMPT_FILE, 
                        help=f"File containing custom instructions to prepend to the prompt (default: {DEFAULT_PREPROMPT_FILE})")
    parser.add_argument("--knowledge", default=DEFAULT_KNOWLEDGE_DIR,
                        help=f"Directory containing knowledge base documents (default: {DEFAULT_KNOWLEDGE_DIR})")
    parser.add_argument("--index", default=DEFAULT_INDEX_NAME,
                        help=f"Name of the vector index to create/use (default: {DEFAULT_INDEX_NAME})")
    parser.add_argument("--rebuild-index", action="store_true",
                        help="Force rebuild of the search index")
    parser.add_argument("--results", type=int, default=3,
                        help="Number of research results to include per article (default: 3)")
    
    args = parser.parse_args()
    
    try:
        # Import glob here to avoid potential issues
        import glob
        
        # Load keywords
        keywords = load_keywords(args.input_file)
        print(f"Loaded {len(keywords)} keywords from {args.input_file}")
        
        # Load preprompt if file exists
        preprompt = load_preprompt(args.preprompt)
        if preprompt:
            print(f"Loaded custom preprompt from {args.preprompt}")
        
        # Initialize RAG components
        print("Initializing RAG components...")
        config = load_config()
        embeddings = initialize_embeddings(config)
        documents = load_documents(args.knowledge)
        vector_db = create_or_load_index(documents, embeddings, args.index, args.rebuild_index)
        
        # Distribute keywords across articles
        article_keyword_sets = distribute_keywords(keywords, args.num_articles)
        print(f"Distributing keywords across {len(article_keyword_sets)} articles")
        
        # Process each article
        for i, article_keywords in enumerate(article_keyword_sets, 1):
            print(f"\n[{i}/{len(article_keyword_sets)}] Generating article with {len(article_keywords)} keywords")
            print(f"Keywords: {', '.join(article_keywords)}")
            
            # Search for relevant research for these keywords
            print("Searching for relevant research...")
            search_query = " ".join(article_keywords)
            research_results = search_documents(search_query, vector_db, k=args.results)
            research_text = format_research_results(research_results)
            
            # Generate content with research data
            content = generate_seo_content(
                keywords=article_keywords,
                research=research_text,
                preprompt=preprompt,
                model=args.model,
                provider=args.provider,
                server_url=args.server
            )
            
            # Save content using the first keyword as primary
            primary_keyword = article_keywords[0]
            filepath = save_content(primary_keyword, article_keywords, content, args.output)
            print(f"Article saved to: {filepath}")
    
    except Exception as e:
        print(f"Error: {e}")
        return 1
    
    print(f"\nSEO content generation complete. {len(article_keyword_sets)} articles saved to {args.output} directory.")
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

This updated version of `articlegenerator.py`:

1. Imports the necessary components from LangChain to support RAG functionality
2. Adapts the core functions from `search.py` to work within this script
3. Adds new command-line arguments for knowledge base location, index management, and search parameters
4. Performs a search for each set of keywords to find relevant research information
5. Incorporates the research into the prompt to generate more authoritative, well-researched content
6. Maintains all existing functionality including preprompt support and keyword distribution

The script now leverages RAG to enhance article quality by pulling in relevant information from a knowledge base, making the generated content more factual and valuable.